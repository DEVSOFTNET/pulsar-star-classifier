{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etiquetamos los nombres de las características en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresNames = ['MeanIntegratedProfile',\n",
    "                'StdIntegratedProfile',\n",
    "                'ExcessKurtosisIntegratedProfile',\n",
    "                'SkewnessIntegratedProfile',\n",
    "                'MeanDMSNRCurve',\n",
    "                'StdDMSNRCurve',\n",
    "                'ExcessKurtosisDMSNRCurve',\n",
    "                'SkewnessDMSNRCurve',\n",
    "                'Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos la informacion del dataset HTRU2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../HTRU_2.csv',\n",
    "                   header = None, \n",
    "                   names = featuresNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos la variable categórica del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('Class', axis = 1, inplace = False)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, existen variables que varian en un rango mayor de valores que otras. Para realizar una mejor comparacion entre las variables es importante estandarizar las variables con una media de cero y una desviación estándar de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "normalized_x = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se divide el dataset en dos conjuntos de datos: entrenamiento y pruebas. El conjunto de entrenamiento estará compuesto por el 90% del dataset y pruebas del 10%.\n",
    "\n",
    "El dataset HTRU2 es un conjunto de datos desbalanceado, por cada púlsar identificado, diez no lo son. Para que esta misma proporción se refleje en los conjuntos de entrenamiento y pruebas se realizará una división estratificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16259\n",
       "1     1639\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(normalized_x,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.1,\n",
    "                                                    random_state = 0,\n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ambos conjuntos de datos tienen la misma proporción con respecto a la variable categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    14633\n",
      "1     1475\n",
      "Name: Class, dtype: int64\n",
      "0    1626\n",
      "1     164\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como mencionamos anteriormente, el dataset HTRU2 es conjunto de datos desbalanceado. Para balancear utilizaremos una técnica de oversampling llamada SMOTE, donde se genera nuevas instancias de la clase minoritaria interpolando los valores de las instancias minoritarias más cercanas a una dada. El nuevo ratio será de 1:5, esto quiere decir, que por cada púlsar, cinco no lo serán en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state = 0,\n",
    "             ratio = 0.2)\n",
    "x_train_smote, y_train_smote = smote.fit_sample(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos observar que el conjunto de datos se encuentra balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14633,  2926])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los mejores hiper-parámetros que se ajusten al modelo utilizaremos la técnica de búsqueda en grilla. \n",
    "\n",
    "Para determinar los mejores valores de hidden layers, activation, solver y max_iter será necesario probar el modelo con diferentes valores.\n",
    "\n",
    "En cada iteración se realizará validación cruzada con k = 5.\n",
    "\n",
    "La métrica de evaluación será f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'hidden_layer_sizes': [(100,), (120,)], 'activation': ['logistic', 'relu'], 'solver': ['lbfgs', 'adam'], 'max_iter': [200, 300, 500], 'tol': [0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {  \n",
    "    'hidden_layer_sizes': [(100, ), (120, )],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'max_iter': [200, 300, 500],\n",
    "    'tol': [0.001]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = MLPClassifier(),\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'f1',\n",
    "                           cv = 5)\n",
    "\n",
    "grid_search.fit(x_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuneando los hiper-parámetros para f1\n",
      "\n",
      "Los mejores hiper-parámetros encontrados con validación cruzada:\n",
      "\n",
      "{'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 200, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "\n",
      "Puntajes de la métrica f1 en el conjunto de validación:\n",
      "\n",
      "0.8973 (+/-0.0195) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 200, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.8964 (+/-0.0132) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 200, 'solver': 'adam', 'tol': 0.001}\n",
      "0.8966 (+/-0.0092) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 300, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.8937 (+/-0.0151) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 300, 'solver': 'adam', 'tol': 0.001}\n",
      "0.8997 (+/-0.0102) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 500, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.8950 (+/-0.0141) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 500, 'solver': 'adam', 'tol': 0.001}\n",
      "0.8939 (+/-0.0165) for {'activation': 'logistic', 'hidden_layer_sizes': (120,), 'max_iter': 200, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.8968 (+/-0.0168) for {'activation': 'logistic', 'hidden_layer_sizes': (120,), 'max_iter': 200, 'solver': 'adam', 'tol': 0.001}\n",
      "0.8927 (+/-0.0165) for {'activation': 'logistic', 'hidden_layer_sizes': (120,), 'max_iter': 300, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.8952 (+/-0.0147) for {'activation': 'logistic', 'hidden_layer_sizes': (120,), 'max_iter': 300, 'solver': 'adam', 'tol': 0.001}\n",
      "0.8954 (+/-0.0173) for {'activation': 'logistic', 'hidden_layer_sizes': (120,), 'max_iter': 500, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.8951 (+/-0.0161) for {'activation': 'logistic', 'hidden_layer_sizes': (120,), 'max_iter': 500, 'solver': 'adam', 'tol': 0.001}\n",
      "0.9126 (+/-0.0090) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 200, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.9134 (+/-0.0132) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 200, 'solver': 'adam', 'tol': 0.001}\n",
      "0.9129 (+/-0.0173) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 300, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.9114 (+/-0.0092) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 300, 'solver': 'adam', 'tol': 0.001}\n",
      "0.9130 (+/-0.0115) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 500, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.9123 (+/-0.0125) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 500, 'solver': 'adam', 'tol': 0.001}\n",
      "0.9138 (+/-0.0093) for {'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 200, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.9106 (+/-0.0170) for {'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 200, 'solver': 'adam', 'tol': 0.001}\n",
      "0.9120 (+/-0.0092) for {'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 300, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.9110 (+/-0.0063) for {'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 300, 'solver': 'adam', 'tol': 0.001}\n",
      "0.9114 (+/-0.0125) for {'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 500, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "0.9135 (+/-0.0144) for {'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 500, 'solver': 'adam', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print('Tuneando los hiper-parámetros para f1')\n",
    "print()\n",
    "\n",
    "print('Los mejores hiper-parámetros encontrados con validación cruzada:')\n",
    "print()\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "print('Puntajes de la métrica f1 en el conjunto de validación:')\n",
    "print()\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print('%0.4f (+/-%0.04f) for %r'\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La búsqueda en grilla nos devuelve el modelo con los mejores hiper-parámetros.\n",
    "\n",
    "Finalmente, vamos a predecir los resultados del modelo con los conjuntos de datos de entrenamiento y pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net_model = grid_search\n",
    "\n",
    "neural_net_model_train_prediction = neural_net_model.predict(x_train_smote)\n",
    "neural_net_model_test_prediction = neural_net_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red neuronal\n",
      "\t-Mejores hiper-parámetros: {'activation': 'relu', 'hidden_layer_sizes': (120,), 'max_iter': 200, 'solver': 'lbfgs', 'tol': 0.001}\n",
      "\t-Puntaje f1 en entrenamiento: 0.9151\n",
      "\t-Puntaje f1 en pruebas: 0.9024\n"
     ]
    }
   ],
   "source": [
    "print('Red neuronal')\n",
    "print('\\t-Mejores hiper-parámetros: ' + str(neural_net_model.best_params_))\n",
    "print('\\t-Puntaje f1 en entrenamiento: %.4f' % f1_score(y_train_smote, neural_net_model_train_prediction, average = 'binary'))\n",
    "print('\\t-Puntaje f1 en pruebas: %.4f' % f1_score(y_test, neural_net_model_test_prediction, average = 'binary'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
